{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0300e19b",
   "metadata": {},
   "source": [
    "Correr una regresión lineal multiple resulta sencillo, puesto que el módulo de <span style=\"background-color: #D3D3D3; padding: 2px;\"> statsmodels</span> tiene implementado una gran cantidad de ssubmódulos, clases y funciones relacionados con la Estadística.\n",
    "Consideremos el siguiente modelo de regresión poblacional:\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... + \\beta_k x_{k,i} + u_i$$\n",
    "\n",
    "Un método un importante que nos interesá sera el método <span style=\"background-color: #D3D3D3; padding: 2px;\"> summary()</span>, ya que este método contendra muchos resultados útiles, tales como las estimaciones de lo parámetros, los errores estandar de la edtiamción, los errores estandar de la regresión, el $R^2$, entre otros que no abordamos aún.\n",
    "\n",
    "Para ejemplificar la estimación del modelo de regresión lineal multiple, se replicara, al igual que el cuaderno 2, los resultados del libro base que tenemos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73f74f8",
   "metadata": {},
   "source": [
    "Sin embargo, antes de continuar es conveniente definir el modelo de Mínimos Cuadrados Ordinarios en su versión matricial.\n",
    "En forma matricial, almacenamos los regresores en una matriz $X$ de tamaño $ n \\times k$ (donde k es el número de parámetros a estimar) que tiene una columna para cada regresor más una columna de unos para la constante. Los valores de muestra de la variable dependiente se almacenan en un vector $Y $ de tamaño $n \\times 1$.\n",
    "\n",
    "La Suma de Cuadrados Residuales se define de la siguiente manera:\n",
    "$$S_T(\\beta) = \\left(Y - X\\beta \\right)' \\left(Y - X \\beta\\right)$$\n",
    "Si minimizamos dicha función podemos llegar a la siguiente ecuación:\n",
    "$$\\hat{\\beta} = (X'X)^{-1}(X'Y)$$\n",
    "\n",
    "Una nota importante, además de otras que no la cubrire, es que para poder estimar $\\beta$, $X'X$ debe ser de rango completo. Es decir no debe haber colinealidad perfecta entre los regresores.\n",
    "\n",
    "El vector de residuos se puede calcular como:\n",
    "$$\\hat{u} = Y - X \\hat{\\beta}$$\n",
    "\n",
    "\n",
    "La fórmula para la varianza estimada del término de error es:\n",
    "$$\\hat{\\sigma}^2 = \\frac{1}{n-k} \\hat{u}' \\hat{u}$$\n",
    "\n",
    "El error estándar de la regresión es su raiz cuadrada $\\sigma = \\sqrt{\\sigma^2}$. La matriz de varianza y covarianza del estimador es la siguiente:\n",
    "$$\\text{Var}(\\hat{\\beta}) = \\hat{\\sigma}^2 (X'X)^{-1}$$\n",
    "Donde, si sacamos la raiz cuadrada a la diagonal principal podremos obtener el error estandar de los estimadores de los parámetros poblacionales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0297bc78",
   "metadata": {},
   "source": [
    "# Data: Determinants of college GPA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5464553",
   "metadata": {},
   "source": [
    "Para este ejemplo tomaremos la base de datos de <font color = red> GPA1 </font>, que relaciona el promedio general de calificaciones de la universidad (colGPA) el promedio general de calificaciones en el bachillerato (hsGPA)\n",
    "y la puntuación en el examen de admisión a la universidad (ACT) para una muestra de 141 estudiantes.\n",
    "El modelo de regresión poblacional propuesto es el siguiente:\n",
    "$$\\text{colGPA} = \\beta_0 + \\beta_1 \\text{hsGPA} + \\beta_3 {ACT} + u$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c973b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import patsy as pt\n",
    "import statsmodels.formula.api as smf\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3a1a0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpa1 = woo.dataWoo('GPA1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12a7e0a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'soph', 'junior', 'senior', 'senior5', 'male', 'campus',\n",
       "       'business', 'engineer', 'colGPA', 'hsGPA', 'ACT', 'job19', 'job20',\n",
       "       'drive', 'bike', 'walk', 'voluntr', 'PC', 'greek', 'car', 'siblings',\n",
       "       'bgfriend', 'clubs', 'skipped', 'alcohol', 'gradMI', 'fathcoll',\n",
       "       'mothcoll'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpa1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "401350bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progamamos lo mencionado anteriormente:\n",
    "# Determinar el tamaño de la muestra y el Nro de regresores\n",
    "n = len(gpa1) \n",
    "k = 3 #No olivar que X contiene una columna de constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c4e069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estraemos Y\n",
    "Y = gpa1['colGPA']\n",
    "# Extraemos X y adicionamos una columna de unos\n",
    "X = pd.DataFrame({'Constants': 1,\n",
    "                 'hsGPA': gpa1['hsGPA'],\n",
    "                 'ACT': gpa1['ACT']})\n",
    "# Forma alternativa con patsy:\n",
    "# y2, X2 = pt.dmatrices(’colGPA ~ hsGPA + ACT’, data=gpa1, return_type=’dataframe’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "100e5ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b: \n",
      "[[1.28632777]\n",
      " [0.45345589]\n",
      " [0.00942601]]\n",
      "\n",
      "SER: [[0.34031576]]\n",
      "\n",
      "se: [0.34082212 0.09581292 0.01077719]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parámetros estimados\n",
    "X = np.array(X)\n",
    "Y = np.array(Y).reshape(n, 1) # lo convertimos en un vector fila\n",
    "b = np.dot(np.linalg.inv(np.dot(X.T, X)), np.dot(X.T, Y))\n",
    "# otra forma interesante de multiplicar matrices:\n",
    "# b1 = np.linalg.inv(X.T @ X) @ X.T @ Y \n",
    "print(f'b: \\n{b}\\n')\n",
    "\n",
    "# residuales, estimamos la varianza de u y su error estandar (SER):\n",
    "u_hat = Y - np.dot(X, b)\n",
    "sigsq_hat = (np.dot(u_hat.T, u_hat)) / (n - k)\n",
    "SER = np.sqrt(sigsq_hat)\n",
    "print(f'SER: {SER}\\n')\n",
    "\n",
    "# estimamos la varianza de los parámetros y su error estandar\n",
    "Vbeta_hat = sigsq_hat * np.linalg.inv(np.dot(X.T, X))\n",
    "se = np.sqrt(np.diagonal(Vbeta_hat))\n",
    "print(f'se: {se}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b372ed8",
   "metadata": {},
   "source": [
    "Podemos comprobar estos resultados obtendidos usando <span style=\"background-color: #D3D3D3; padding: 2px;\"> statsmodels</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9e577e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usar summary(): \n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 colGPA   R-squared:                       0.176\n",
      "Model:                            OLS   Adj. R-squared:                  0.164\n",
      "Method:                 Least Squares   F-statistic:                     14.78\n",
      "Date:                Fri, 01 Mar 2024   Prob (F-statistic):           1.53e-06\n",
      "Time:                        21:44:03   Log-Likelihood:                -46.573\n",
      "No. Observations:                 141   AIC:                             99.15\n",
      "Df Residuals:                     138   BIC:                             108.0\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      1.2863      0.341      3.774      0.000       0.612       1.960\n",
      "hsGPA          0.4535      0.096      4.733      0.000       0.264       0.643\n",
      "ACT            0.0094      0.011      0.875      0.383      -0.012       0.031\n",
      "==============================================================================\n",
      "Omnibus:                        3.056   Durbin-Watson:                   1.885\n",
      "Prob(Omnibus):                  0.217   Jarque-Bera (JB):                2.469\n",
      "Skew:                           0.199   Prob(JB):                        0.291\n",
      "Kurtosis:                       2.488   Cond. No.                         298.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculamos la función de regresión MCO:\n",
    "reg = smf.ols(formula='colGPA ~ hsGPA + ACT', data=gpa1)\n",
    "results = reg.fit()\n",
    "print(f'Usar summary(): \\n{results.summary()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3c84d6",
   "metadata": {},
   "source": [
    "## Omisión de variables relevantes\n",
    "Omitir regresores relevantes causa sesgo si estamos interesados en la estimación de efectos parciales. TAmbién como se vera más adelante, omitir variables relevantes causa que nuestros estimadores no sean consistentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3bd183f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_tilde: \n",
      "Intercept    2.462537\n",
      "ACT          0.038897\n",
      "dtype: float64\n",
      "\n",
      "b1_tilde: \n",
      "0.027063973943178526\n",
      "\n",
      "b_om: \n",
      "Intercept    2.402979\n",
      "ACT          0.027064\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b = results.params\n",
    "# relación entre regresores:\n",
    "reg_delta = smf.ols(formula='hsGPA ~ ACT', data=gpa1)\n",
    "results_delta = reg_delta.fit()\n",
    "delta_tilde = results_delta.params\n",
    "print(f'delta_tilde: \\n{delta_tilde}\\n')\n",
    "# Fórmula de variables omitidas para b1_tilde:\n",
    "b1_tilde = b['ACT'] + b['hsGPA'] * delta_tilde['ACT']\n",
    "print(f'b1_tilde: \\n{b1_tilde}\\n')\n",
    "# regresión real con hsGPA omitida:\n",
    "reg_om = smf.ols(formula='colGPA ~ ACT', data=gpa1)\n",
    "results_om = reg_om.fit()\n",
    "b_om = results_om.params\n",
    "print(f'b_om: \\n{b_om}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58f8115",
   "metadata": {},
   "source": [
    "## Errores estándar, multicolinealidad y VIF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ced94d",
   "metadata": {},
   "source": [
    "Ya hemos visto la fórmula matricial para la matriz de varianza-covarianza condicional bajo los supuestos habituales, incluida la homocedasticidad (MLR.5). El supuesto de homocedasticidad nos proporciona una fórmula útil para la varianza de un solo parámetro $\\beta_j$, es decir, para un solo elemento en el principal diagonal de la matriz de varianza-covarianza:\n",
    "$$\\text{Var}(\\beta_j) = \\frac{\\sigma^2}{SCT_j (1 - R_j^2)} = \\frac{1}{n} \\cdot \\frac{\\sigma^2}{\\text{Var}(x_j)} \\cdot \\frac{1}{1 - R_j^2}$$\n",
    "\n",
    "Donde $SCT = \\sum_{i=1}^n (x_{ji} - \\bar{x_j})^2 = (n-1) \\cdot \\text{Var}(x_j)$ es la suma total de cuadrados y $R^2$ es el coeficiente habitual de determinación de una regresión de $x_j$ en todos los demás regresores.\n",
    "\n",
    "La varianza de $\\beta_j$ consiste en cuatro partes:\n",
    "- $\\frac{1}{n}$: la varianza es menor a medida que las muestra aumenta\n",
    "- $\\sigma^2$: La varianza es mayor si el término de error varía mucho, ya que introduce aleatoriedad en el relación entre las variables de interés.\n",
    "- $\\frac{1}{\\text{Var}(x_j)}$: La varianza es menor si el regresor $x_j$ varía mucho ya que esto proporciona información relevante sobre la relación.\n",
    "- $\\frac{1}{1 - R_j^2}$: Este factor de inflación de varianza (VIF) explica la multicolinealidad (imperfecta). Si $x_j$ está altamente relacionado con los otros regresores, $R^2_j$ y por lo tanto también $\\text{VIF}_j$ y la varianza de $b_j$ son grandes.\n",
    "\n",
    "Dado que se desconoce la varianza del error $\\sigma^2$, la reemplazamos con una estimación para obtener una varianza estimada de la estimación del parámetro.\n",
    "\n",
    "La raiz cuadrada de $\\text{Var}(\\beta_j)$ es el error estandar:\n",
    "\n",
    "$$se(\\beta_j) = \\frac{1}{\\sqrt{n}} \\cdot \\frac{\\hat{\\sigma}^2}{\\text{SD}(x_j)} \\cdot \\frac{1}{\\sqrt{1 - R_j^2}}$$\n",
    "\n",
    "Por lo tanto, podemos obtener estos resultados tanto de forma manual, como también usando los métodos y propiedades del módulo <span style=\"background-color: #D3D3D3; padding: 2px;\"> statsmodels.formula.api </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "489f1f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIF_hsGPA: \n",
      "1.1358234481972784\n",
      "\n",
      "SE_hsGPA: \n",
      "0.09581291608057602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Continuamos usando los resultados obtenidos anteriormente\n",
    "\n",
    "# extraer SER (error estandar de los residuos):\n",
    "SER = np.sqrt(results.mse_resid)\n",
    "\n",
    "# Regresión de hsGPA contra ACT para el cálculo de R2 y VIF:\n",
    "reg_hsGPA = smf.ols(formula='hsGPA ~ ACT', data=gpa1)\n",
    "results_hsGPA = reg_hsGPA.fit()\n",
    "R2_hsGPA = results_hsGPA.rsquared\n",
    "VIF_hsGPA = 1 / (1 - R2_hsGPA)\n",
    "print(f'VIF_hsGPA: \\n{VIF_hsGPA}\\n')\n",
    "# Cálculo manual del error estandar (se) del coeficiente hsGPA:\n",
    "n = results.nobs\n",
    "sdx = np.std(gpa1['hsGPA'], ddof=1) * np.sqrt((n - 1) / n)\n",
    "SE_hsGPA = 1 / np.sqrt(n) * SER / sdx * np.sqrt(VIF_hsGPA)\n",
    "print(f'SE_hsGPA: \\n{SE_hsGPA}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acb0ee6",
   "metadata": {},
   "source": [
    "El módulo <span style=\"background-color: #D3D3D3; padding: 2px;\"> statsmodels </span> en <span style=\"background-color: #D3D3D3; padding: 2px;\"> stats.outliers_influence </span> proporciona una manera conveniente de calcular automáticamente los factores de inflación de varianza (VIF). El comando<span style=\"background-color: #D3D3D3; padding: 2px;\"> variance_inflation_factor </span> entrega el VIF para una matriz X y el número de un regresor determinado (comenzando con la constante como regresor con el número 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "961a10b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIF: \n",
      "[141.41990589   1.13582345   1.13582345]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import patsy as pt\n",
    "import statsmodels.stats.outliers_influence as smo\n",
    "\n",
    "y, X = pt.dmatrices('colGPA ~ hsGPA + ACT',\n",
    "                    data=gpa1, return_type='dataframe')\n",
    "# Conseguir VIF:\n",
    "K = X.shape[1] #Regresores, 2 VI mas el intercpeto, 3\n",
    "VIF = np.empty(K) \n",
    "for i in range(K):\n",
    "    VIF[i] = smo.variance_inflation_factor(X.values, i)\n",
    "print(f'VIF: \\n{VIF}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5118836",
   "metadata": {},
   "source": [
    "Lo cual nos indica que nuestras variables regreseras no presentan alta correlación, por lo tanto no hay que preocuparnos por la multicolinealidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914aa685",
   "metadata": {},
   "source": [
    "# Data: equation for hourly wage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c12be6",
   "metadata": {},
   "source": [
    "Para este segundo ejemplo se usara la base de datos <font color = red> WAGE1 </font>. Donde relacionamos los años de aducación (educ), años de experiencia en el mercado laboral (exper) y los años de antiguedad en el empleo actual (tenure) para explicar el salario (wage). Para una muestra de 526 trabajadores.\n",
    "Proponemos el siguiente modelo de regresión poblacional:\n",
    "\n",
    "$$\\text{wage} = \\beta_0 + \\beta_1 \\text{educ} + \\beta_2 \\text{exper} + \\beta_3 \\text{tenure} + u$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdf27633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base de datos\n",
    "wage1 = woo.dataWoo('WAGE1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d290bd1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['wage', 'educ', 'exper', 'tenure', 'nonwhite', 'female', 'married',\n",
       "       'numdep', 'smsa', 'northcen', 'south', 'west', 'construc', 'ndurman',\n",
       "       'trcommpu', 'trade', 'services', 'profserv', 'profocc', 'clerocc',\n",
       "       'servocc', 'lwage', 'expersq', 'tenursq'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wage1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "627cd21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados: \n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           np.log(wage)   R-squared:                       0.316\n",
      "Model:                            OLS   Adj. R-squared:                  0.312\n",
      "Method:                 Least Squares   F-statistic:                     80.39\n",
      "Date:                Fri, 01 Mar 2024   Prob (F-statistic):           9.13e-43\n",
      "Time:                        21:44:03   Log-Likelihood:                -313.55\n",
      "No. Observations:                 526   AIC:                             635.1\n",
      "Df Residuals:                     522   BIC:                             652.2\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.2844      0.104      2.729      0.007       0.080       0.489\n",
      "educ           0.0920      0.007     12.555      0.000       0.078       0.106\n",
      "exper          0.0041      0.002      2.391      0.017       0.001       0.008\n",
      "tenure         0.0221      0.003      7.133      0.000       0.016       0.028\n",
      "==============================================================================\n",
      "Omnibus:                       11.534   Durbin-Watson:                   1.769\n",
      "Prob(Omnibus):                  0.003   Jarque-Bera (JB):               20.941\n",
      "Skew:                           0.021   Prob(JB):                     2.84e-05\n",
      "Kurtosis:                       3.977   Cond. No.                         135.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Regresionamos el modelo MCO\n",
    "# Usar un modelo semi log, donde transformamos la variable wage en log\n",
    "reg = smf.ols(formula='np.log(wage) ~ educ + exper + tenure', data=wage1)\n",
    "results = reg.fit()\n",
    "print(f'Resultados: \\n{results.summary()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b7cf135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIF: \n",
      "[29.37890286  1.11277075  1.47761777  1.34929556]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Observando la multicolinealidad\n",
    "\n",
    "# Extraer matrices usando patsi\n",
    "y, X = pt.dmatrices('np.log(wage) ~ educ + exper + tenure',\n",
    "                    data=wage1, return_type='dataframe')\n",
    "# get VIF:\n",
    "K = X.shape[1]\n",
    "VIF = np.empty(K)\n",
    "for i in range(K):\n",
    "    VIF[i] = smo.variance_inflation_factor(X.values, i)\n",
    "print(f'VIF: \\n{VIF}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.844px",
    "left": "996px",
    "right": "20px",
    "top": "122px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
